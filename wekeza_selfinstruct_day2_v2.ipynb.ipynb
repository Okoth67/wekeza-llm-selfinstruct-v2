{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "963e2760-c7f9-435e-860e-1109192704f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Explain what a Money Market Fund is.',\n",
       " 'input': '',\n",
       " 'output': 'A Money Market Fund (MMF) is a type of investment that pools money from investors to purchase short-term, low-risk financial securities such as Treasury bills, commercial paper, and certificates of deposit. In Kenya, MMFs are popular because they offer higher returns than regular savings accounts while maintaining liquidity and low risk.'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#manually written instruction - output examples\n",
    "{\"instruction\": \"Explain what a Money Market Fund is.\", \"input\": \"\", \"output\": \"A Money Market Fund (MMF) is a type of investment that pools money from investors to purchase short-term, low-risk financial securities such as Treasury bills, commercial paper, and certificates of deposit. In Kenya, MMFs are popular because they offer higher returns than regular savings accounts while maintaining liquidity and low risk.\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1b04ba1-b63f-4cb0-a3e2-2020c869d091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bbollo\\AppData\\Local\\anaconda3\\envs\\wekeza\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "#loading my finetuned model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "model_path = \"./distilgpt2-wekeza-finetuned_v1\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a47fffe5-da2b-4c60-8394-4caca871f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed instructionn for self generation\n",
    "seed_instructions = [\n",
    "    \"Explain how interest is earned in a Money Market Fund in Kenya.\",\n",
    "    \"Describe the risks of investing in the Nairobi Securities Exchange.\",\n",
    "    \"Give steps to start investing in government bonds in Kenya.\",\n",
    "    \"List advantages of investing in a Unit Trust compared to fixed deposits.\",\n",
    "    \"Explain how a SACCO works and how it benefits its members.\",\n",
    "    \"Describe how to calculate compound interest for a savings product.\",\n",
    "    \"Explain what the yield curve tells us about Kenya’s economy.\",\n",
    "    \"Give tips for new investors entering the Kenyan stock market.\",\n",
    "    \"Describe the difference between a money market fund and a mutual fund.\",\n",
    "    \"Explain how a Treasury bill differs from a Treasury bond.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19e67177-918b-4d24-a622-9276f907730b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2.jsonl created with 5 custom instructions.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "seed_data = [\n",
    "    {\n",
    "        \"instruction\": \"What is a money market fund and how does it work in Kenya?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"A money market fund in Kenya is a type of unit trust that pools investors’ money and invests in low-risk, short-term financial instruments like treasury bills, fixed deposits, and commercial papers. It is popular due to its liquidity and interest returns higher than typical savings accounts.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Compare CIC Money Market Fund and NCBA Money Market Fund in terms of returns and accessibility.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"CIC's Money Market Fund offers competitive rates with low entry barriers (minimum KES 500) and mobile top-up. NCBA's MMF offers similar accessibility via Loop or M-Pesa, though CIC often publishes slightly higher daily yields. Both are regulated by CMA.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"How do I calculate expected interest from a money market fund?\",\n",
    "        \"input\": \"I have KES 100,000 and the average daily yield is 9.5% annualized.\",\n",
    "        \"output\": \"To estimate monthly interest: (KES 100,000 x 0.095) ÷ 12 = approx. KES 791.67. Actual interest depends on daily compounding and fees, so always check with the fund manager.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain the risk profile of money market funds to a beginner.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Money market funds are considered low-risk because they invest in short-term, government-backed or highly-rated financial instruments. They’re suitable for conservative investors or those looking for stable returns.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"How can I withdraw funds from a money market fund in Kenya?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Most MMFs in Kenya allow withdrawals via mobile money (like M-Pesa) or bank transfer. You usually get funds within 24–72 hours after submitting a withdrawal request.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "with open(\"v2.jsonl\", \"w\") as f:\n",
    "    for item in seed_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(\"v2.jsonl created with 5 custom instructions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5babf37-4ebd-4fea-9ec5-77935f4e6158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:03<00:00, 12.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 15 synthetic samples to self_instruct_raw_v2.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "#loading the seeded instructions\n",
    "with open(\"v2.jsonl\", \"r\") as f:\n",
    "    seed_data = [json.loads(l) for l in f.readlines()]\n",
    "\n",
    "synthetic_data = []\n",
    "\n",
    "generations_per_seed = 3\n",
    "\n",
    "#getting new samples\n",
    "for example in tqdm(seed_data):\n",
    "    instruction = example[\"instruction\"]\n",
    "    input_text = example.get(\"input\", \"\")\n",
    "    prompt = f\"\"\"Below is an instruction. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "    #generating n completions\n",
    "    outputs = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=200,\n",
    "        num_return_sequences=generations_per_seed,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "\n",
    "    #formating and saving the new generated promps\n",
    "    for out in outputs:\n",
    "        response = out[\"generated_text\"].split(\"### Response:\")[-1].strip()\n",
    "        synthetic_data.append({\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": input_text,\n",
    "            \"output\": response\n",
    "        })\n",
    "\n",
    "#this will be the new json file\n",
    "with open(\"self_instruct_raw_v2.jsonl\", \"w\") as f:\n",
    "    for item in synthetic_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(synthetic_data)} synthetic samples to self_instruct_raw_v2.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2b04c44-405c-4e97-8965-496f94d28069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to v2_cleaned.jsonl with 14 samples.\n"
     ]
    }
   ],
   "source": [
    "#Cleaning and filtering\n",
    "import json\n",
    "\n",
    "input_file = \"self_instruct_raw_v2.jsonl\"\n",
    "output_file = \"v2_cleaned.jsonl\"\n",
    "\n",
    "with open(input_file, \"r\") as infile:\n",
    "    raw_data = [json.loads(line) for line in infile]\n",
    "\n",
    "cleaned_data = []\n",
    "seen = set()\n",
    "\n",
    "for example in raw_data:\n",
    "    instruction = example[\"instruction\"].strip()\n",
    "    input_text = example[\"input\"].strip()\n",
    "    output = example[\"output\"].strip()\n",
    "\n",
    "    if len(output) < 20:\n",
    "        continue\n",
    "    if (instruction, input_text, output) in seen:\n",
    "        continue\n",
    "    if not output:\n",
    "        continue\n",
    "\n",
    "    seen.add((instruction, input_text, output))\n",
    "    cleaned_data.append({\n",
    "        \"instruction\": instruction,\n",
    "        \"input\": input_text,\n",
    "        \"output\": output\n",
    "    })\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    for item in cleaned_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"Cleaned dataset saved to {output_file} with {len(cleaned_data)} samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d590fea7-b3d3-458d-af84-112dafd5c8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset saved to WekezaLLM_dataset_v2.jsonl with 142 total samples.\n"
     ]
    }
   ],
   "source": [
    "#merging the dataset\n",
    "import json\n",
    "\n",
    "v1_file = r\"C:\\Users\\bbollo\\Downloads\\WekezaLLM_dataset_v1.jsonl\"\n",
    "v2_file = \"v2_cleaned.jsonl\"\n",
    "merged_file = \"WekezaLLM_dataset_v2.jsonl\"\n",
    "\n",
    "with open(v1_file, \"r\") as f:\n",
    "    v1_data = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "with open(v2_file, \"r\") as f:\n",
    "    v2_data = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "combined = v1_data + v2_data\n",
    "\n",
    "#saving the new merged file\n",
    "with open(merged_file, \"w\") as f:\n",
    "    for item in combined:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"Combined dataset saved to {merged_file} with {len(combined)} total samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98bf180d-0519-4fd8-913d-3b12435c1c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing the merged dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"./distilgpt2-wekeza-finetuned_v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cb7a578-1a46-48ad-b538-c8b130cb1527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'What is the minimum amount I need to start investing in a money market fund in Kenya?',\n",
       " 'input': '',\n",
       " 'output': 'Most money market funds in Kenya have a minimum investment of KES 1,000 to KES 5,000, with some like CIC Money Market Fund starting at KES 1,000. Popular funds from Britam, Old Mutual, and ICEA allow you to start with as little as KES 1,000 and make additional contributions of KES 500 or more.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading new merged dataset\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "input_path = \"WekezaLLM_dataset_v2.jsonl\"\n",
    "\n",
    "data = []\n",
    "with open(input_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "#change to HuggingFace Dataset\n",
    "dataset = Dataset.from_list(data)\n",
    "dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7305bd76-2057-459a-9916-774cadc06c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 142 examples [00:00, 4258.54 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 2504.43 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 931.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#tokenizing the dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"WekezaLLM_dataset_v2.jsonl\")[\"train\"]\n",
    "\n",
    "def format_prompt(example):\n",
    "    instruction = example[\"instruction\"]\n",
    "    input_text = example[\"input\"]\n",
    "    output_text = example[\"output\"]\n",
    "\n",
    "    if input_text.strip():\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output_text}\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output_text}\"\n",
    "    \n",
    "    return {\"text\": prompt}\n",
    "\n",
    "formatted_dataset = dataset.map(format_prompt)\n",
    "\n",
    "#actual tokenization\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "tokenized_dataset = formatted_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c436c1e6-fa6a-4a24-81e1-380c013ab6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bbollo\\AppData\\Local\\Temp\\ipykernel_16376\\3009392178.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "#finetuing setting up\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "output_dir = \"./distilgpt2-wekeza-finetuned_v2\"\n",
    "\n",
    "#TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=10,\n",
    "    fp16=False,\n",
    ")\n",
    "\n",
    "# Data collator for causal LM\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1321bd7e-3a39-44d0-98ca-2a451d2c30ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bbollo\\AppData\\Local\\anaconda3\\envs\\wekeza\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='426' max='426' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [426/426 38:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.409300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.139900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.334400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.346600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.083400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.472500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.914200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.674200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.521900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.580800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.538000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.352800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.769900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.853600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.301100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.592400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.168200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.635800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.411200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.202300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.592700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.200300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.468800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.224500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.236200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.022100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.858400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.908600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.942100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.250200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.787000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.407000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.127900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.337100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.988200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>2.198100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.195700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bbollo\\AppData\\Local\\anaconda3\\envs\\wekeza\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\bbollo\\AppData\\Local\\anaconda3\\envs\\wekeza\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=426, training_loss=2.3106516426158064, metrics={'train_runtime': 2309.2097, 'train_samples_per_second': 0.184, 'train_steps_per_second': 0.184, 'total_flos': 55656207876096.0, 'train_loss': 2.3106516426158064, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fine tunin\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02fadc97-4bb0-424d-9516-0e7a86c8756b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./distilgpt2-wekeza-finetuned_v2\\\\tokenizer_config.json',\n",
       " './distilgpt2-wekeza-finetuned_v2\\\\special_tokens_map.json',\n",
       " './distilgpt2-wekeza-finetuned_v2\\\\vocab.json',\n",
       " './distilgpt2-wekeza-finetuned_v2\\\\merges.txt',\n",
       " './distilgpt2-wekeza-finetuned_v2\\\\added_tokens.json',\n",
       " './distilgpt2-wekeza-finetuned_v2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving the finetuned model and its tokenizer\n",
    "save_path = \"./distilgpt2-wekeza-finetuned_v2\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f1342-cbbb-478e-80a9-e20905912cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
